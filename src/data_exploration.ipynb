{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e6d4c7-99f9-451d-89ea-d8b6792787f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (model.py, line 231)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3550\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\n\u001b[0;31m    from model import TranSiGen\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/TranSiGen/src/model.py:231\u001b[0;36m\u001b[0m\n\u001b[0;31m    train_mse_x1 = train_dict['mse_x1']aaah\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dataset import TranSiGenDataset\n",
    "from model import TranSiGen\n",
    "from utils import *\n",
    "import pickle\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c7f9-a187-4a01-a35a-a32977d1d3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a3770-52b2-4ed4-bf88-909605a4e307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b5fac-b987-4788-91d2-1918389fbb38",
   "metadata": {},
   "source": [
    "## Carregamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07e182-b0fe-4469-9304-278ec6033d2c",
   "metadata": {},
   "source": [
    "Os dados são carregados a partir de um arquivo `.h5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12173e34-cf50-404b-a0dd-39e273810c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '../data/LINCS2020/data_example/processed_data_id.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de094c-4fff-47de-81bf-f4ca69884ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_from_HDF(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617fe057-26e0-4a6d-a11e-cf1482a19735",
   "metadata": {},
   "source": [
    "O conteúdo do arquivo é um dicionário contendo as seguintes informações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a050cb-6f03-41c7-8196-e9baa1d37ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\", \".join(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc0c3d-6e66-45be-b6d3-d80b0bd3c969",
   "metadata": {},
   "source": [
    "- **LINCS_index**: \n",
    "- **canonical_smiles**: ids das moléculas dos compostos químicos. Esse id é convertido para o padrão Smiles usando o vetor `idx2smi` definido posteriormente.\n",
    "- **cid**: cell line identifier\n",
    "- **sig**: signature\n",
    "\n",
    "Esse conjunto é, em resumo, um ponteiro para os dados. Os dados em si serão carregados mais à frente pela classe `TranSiGenDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967a1b4-595c-4c78-bc99-ddd8ce91a279",
   "metadata": {},
   "source": [
    "## Definição das configurações gerais\n",
    "Observações:\n",
    "- `cell_count`: número de linhagens celulares. Usado apenas para identificar o modelo salvo.\n",
    "- `feat_type`: tipo de representação das moléculas. Para o uso no modelo, o código Smiles é convertido para uma outra representação da molécula. Essa conversão pode ser feita tanto para um embedding pelo modelo pré-treinado KPGT, ou pela impressão digital molecular (*molecular fingerprint*) ECFP4.\n",
    "- `split_type`: define como os dados serão dividos em treino, validação e teste. Os possíveis valores são\n",
    "    - `random_split`: essa divisão é feita de forma aleatória.\n",
    "    - `cell_split`: os conjuntos são dividos considerando uma mesma linhagem celular, para que não haja o risco de todos os dados de uma célula (ou boa parte deles) fiquem apenas em um dos conjuntos.\n",
    "- `features_dim`: tamanho do vetor que representa a molécula. No caso do KPGT, esse valor é 2304. No caso do ECFP4, o valor é 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7069c-fe3b-4a52-8785-6e22f1d320d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_count = len(set(data['cid']))\n",
    "feat_type = 'KPGT'\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "beta = 0.1\n",
    "dropout = 0.1\n",
    "weight_decay = 1e-5\n",
    "n_folds = 5\n",
    "random_seed = 364039\n",
    "split_type = 'smiles_split'\n",
    "features_dim = 2304\n",
    "features_embed_dim = [400]\n",
    "n_latent = 100\n",
    "init_mode = 'pretrain_shRNA'\n",
    "n_epochs = 300\n",
    "molecule_path = '../data/LINCS2020/idx2smi.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "160ef299-49dc-4693-8f1d-4bf77aeccfe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_out = '../results/trained_models_{}_cell_{}/{}/feature_{}_init_{}/'.format(cell_count, split_type, random_seed, feat_type, init_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed0efd-6159-4f5e-a069-4281a2da83a3",
   "metadata": {},
   "source": [
    "Abaixo é carregador o vetor `idx2smi`, responsável por converter os índices das moléculas carregadas acima nos respectivos códigos Smiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a694b540-38aa-4493-bb08-313971d2b725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(molecule_path, 'rb') as f:\n",
    "    idx2smi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66385d-f435-43c8-b517-af6dd5dd98f4",
   "metadata": {},
   "source": [
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93cbc822-0c44-4248-80ba-3e43fc3b9c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BrCC(=O)NCCc1ccc2ccccc2c1'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2smi[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1c83e-df27-4e69-8b9e-5102e676e503",
   "metadata": {},
   "source": [
    "## Divisão de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c2d21ad-e3d6-4c29-b8c4-75df8238c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair, pairv, pairt = split_data(data, n_folds=n_folds, split_type=split_type, rnds=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5820a600-8842-4546-8847-7b26bb0844d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = TranSiGenDataset(\n",
    "    LINCS_index=pair['LINCS_index'],\n",
    "    mol_feature_type=feat_type,\n",
    "    mol_id=pair['canonical_smiles'],\n",
    "    cid=pair['cid']\n",
    ")\n",
    "\n",
    "valid = TranSiGenDataset(\n",
    "    LINCS_index=pairv['LINCS_index'],\n",
    "    mol_feature_type=feat_type,\n",
    "    mol_id=pairv['canonical_smiles'],\n",
    "    cid=pairv['cid']\n",
    ")\n",
    "\n",
    "test = TranSiGenDataset(\n",
    "    LINCS_index=pairt['LINCS_index'],\n",
    "    mol_feature_type=feat_type,\n",
    "    mol_id=pairt['canonical_smiles'],\n",
    "    cid=pairt['cid']\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=4, worker_init_fn=seed_worker)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=4, worker_init_fn=seed_worker)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=4, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6aef5-5300-44e7-a38c-084f914f4f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Criação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd82803d-58fc-4a3b-8086-2a85c1c5c0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TranSiGen(\n",
    "    n_genes=978,\n",
    "    n_latent=n_latent,\n",
    "    n_en_hidden=[1200],\n",
    "    n_de_hidden=[800],\n",
    "    features_dim=features_dim,\n",
    "    features_embed_dim=features_embed_dim,\n",
    "    init_w=True,\n",
    "    beta=beta,\n",
    "    device=dev,\n",
    "    dropout=dropout,\n",
    "    path_model=local_out,\n",
    "    random_seed=random_seed\n",
    ")\n",
    "_ = model.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857502d-25ef-4116-95c5-78bd82ba17b8",
   "metadata": {},
   "source": [
    "### Arquitetura do Modelo\n",
    "Arquitetura dos codificadores do $X_1$ e do $X_2$ (são iguais):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a2429602-2790-4490-b42b-577cee434d80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=978, out_features=1200, bias=True)\n",
       "  (1): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder_x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434303f-071e-470a-9c3c-f41e94070f2f",
   "metadata": {},
   "source": [
    "Arquitetura dos decodificadores do $X_1$ e do $X_2$ (também são iguais):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6482e22-f46f-4d5e-8a1e-230fcc99e184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=800, bias=True)\n",
       "  (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       "  (4): Linear(in_features=800, out_features=978, bias=True)\n",
       "  (5): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7dbe8-68b6-4104-aa0f-0e0e7c3514bd",
   "metadata": {},
   "source": [
    "Arquitetura do embedder da molécula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4682f557-fa5a-49d3-b69e-5ff064c0cf72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2304, out_features=400, bias=True)\n",
       "  (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feat_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d100118-bf7f-4fa8-a9d3-823361903d81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Treinamento/Carregamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15acfc55-6fc3-4c6e-8289-e269046ed0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "26553013-91bd-4e5a-9d27-24583958a975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "7852d3d7-2838-4f93-acec-2b200486f048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoisePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.profile_encoder = nn.Sequential(\n",
    "            nn.Linear(978, 100),\n",
    "            nn.BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.1, inplace=False),\n",
    "        )\n",
    "        self.molecule_encoder = nn.Sequential(\n",
    "            nn.Linear(2304, 100),\n",
    "            nn.BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.1, inplace=False),\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(200, 500),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(500, 978),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.attention = SelfAttention(2)\n",
    "    \n",
    "    def forward(self, profile, molecule):\n",
    "        prof_code = self.profile_encoder(profile)\n",
    "        mol_code = self.molecule_encoder(molecule)\n",
    "        code = torch.stack((prof_code, mol_code), dim=2)\n",
    "        code = self.attention(code)\n",
    "        code = code.view(-1, 200)\n",
    "        prediction = self.final_predictor(code)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "5ec1e9a6-18f0-4a6e-8392-866ac47e734b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = NoisePredictor().to(dev)\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c23888b6-9a8c-4245-805d-6395d50aa80f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "12e3392e-1e62-4b9b-ae46-b5c08bbbe1ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train Loss 2307784.25. Valid Loss 1343727.0\n",
      "Epoch 2. Train Loss 2234564.75. Valid Loss 1311396.25\n",
      "Epoch 3. Train Loss 2158085.5. Valid Loss 1272578.0\n",
      "Epoch 4. Train Loss 2074556.875. Valid Loss 1231739.0\n",
      "Epoch 5. Train Loss 1997476.0. Valid Loss 1191173.25\n",
      "Epoch 6. Train Loss 1928593.5. Valid Loss 1151740.25\n",
      "Epoch 7. Train Loss 1853993.5. Valid Loss 1113665.75\n",
      "Epoch 8. Train Loss 1783243.625. Valid Loss 1076097.5\n",
      "Epoch 9. Train Loss 1719605.625. Valid Loss 1039460.5\n",
      "Epoch 10. Train Loss 1659318.25. Valid Loss 999664.875\n",
      "Epoch 11. Train Loss 1610707.625. Valid Loss 957700.125\n",
      "Epoch 12. Train Loss 1559925.875. Valid Loss 919547.0\n",
      "Epoch 13. Train Loss 1507543.0. Valid Loss 884051.1875\n",
      "Epoch 14. Train Loss 1466795.25. Valid Loss 856646.25\n",
      "Epoch 15. Train Loss 1428699.5. Valid Loss 834972.625\n",
      "Epoch 16. Train Loss 1397911.75. Valid Loss 821297.75\n",
      "Epoch 17. Train Loss 1371658.75. Valid Loss 813588.625\n",
      "Epoch 18. Train Loss 1347588.75. Valid Loss 811043.5\n",
      "Epoch 19. Train Loss 1319266.5. Valid Loss 812118.9375\n",
      "Epoch 20. Train Loss 1303003.375. Valid Loss 813517.75\n",
      "Epoch 21. Train Loss 1284792.75. Valid Loss 812733.1875\n",
      "Epoch 22. Train Loss 1278783.375. Valid Loss 804693.8125\n",
      "Epoch 23. Train Loss 1266951.25. Valid Loss 792807.8125\n",
      "Epoch 24. Train Loss 1256054.5. Valid Loss 779502.3125\n",
      "Epoch 25. Train Loss 1245802.5. Valid Loss 767671.75\n",
      "Epoch 26. Train Loss 1240531.75. Valid Loss 757432.25\n",
      "Epoch 27. Train Loss 1240135.0. Valid Loss 751699.0625\n",
      "Epoch 28. Train Loss 1237070.5. Valid Loss 747686.875\n",
      "Epoch 29. Train Loss 1230750.25. Valid Loss 744860.375\n",
      "Epoch 30. Train Loss 1233540.75. Valid Loss 743760.75\n",
      "Epoch 31. Train Loss 1231751.75. Valid Loss 742976.8125\n",
      "Epoch 32. Train Loss 1229826.25. Valid Loss 742313.875\n",
      "Epoch 33. Train Loss 1229218.0. Valid Loss 741846.625\n",
      "Epoch 34. Train Loss 1228695.25. Valid Loss 741884.0\n",
      "Epoch 35. Train Loss 1228371.0. Valid Loss 743038.5625\n",
      "Epoch 36. Train Loss 1226310.75. Valid Loss 743365.625\n",
      "Epoch 37. Train Loss 1227882.75. Valid Loss 744083.5\n",
      "Epoch 38. Train Loss 1225487.0. Valid Loss 743823.0\n",
      "Epoch 39. Train Loss 1225111.625. Valid Loss 744358.875\n",
      "Epoch 40. Train Loss 1226483.75. Valid Loss 745520.0\n",
      "Epoch 41. Train Loss 1225420.0. Valid Loss 746930.625\n",
      "Epoch 42. Train Loss 1224756.75. Valid Loss 749829.25\n",
      "Epoch 43. Train Loss 1221635.0. Valid Loss 749860.4375\n",
      "Epoch 44. Train Loss 1222215.5. Valid Loss 751748.1875\n",
      "Epoch 45. Train Loss 1220435.75. Valid Loss 751460.1875\n",
      "Epoch 46. Train Loss 1219555.5. Valid Loss 752742.875\n",
      "Epoch 47. Train Loss 1216866.25. Valid Loss 751546.3125\n",
      "Epoch 48. Train Loss 1218287.0. Valid Loss 749714.9375\n",
      "Epoch 49. Train Loss 1220281.625. Valid Loss 749017.625\n",
      "Epoch 50. Train Loss 1216851.75. Valid Loss 748708.5625\n",
      "Epoch 51. Train Loss 1217977.0. Valid Loss 752319.375\n",
      "Epoch 52. Train Loss 1216279.5. Valid Loss 755731.25\n",
      "Epoch 53. Train Loss 1218023.375. Valid Loss 761088.8125\n",
      "Epoch 54. Train Loss 1216330.5. Valid Loss 763207.125\n",
      "Epoch 55. Train Loss 1216178.625. Valid Loss 764252.875\n",
      "Epoch 56. Train Loss 1215758.0. Valid Loss 759166.6875\n",
      "Epoch 57. Train Loss 1216066.75. Valid Loss 753483.125\n",
      "Epoch 58. Train Loss 1215974.75. Valid Loss 750714.0\n",
      "Epoch 59. Train Loss 1217340.25. Valid Loss 744293.375\n",
      "Epoch 60. Train Loss 1214515.0. Valid Loss 737503.625\n",
      "Epoch 61. Train Loss 1216156.625. Valid Loss 734716.875\n",
      "Epoch 62. Train Loss 1214106.75. Valid Loss 733028.125\n",
      "Epoch 63. Train Loss 1212311.625. Valid Loss 732124.8125\n",
      "Epoch 64. Train Loss 1217968.0. Valid Loss 731676.875\n",
      "Epoch 65. Train Loss 1213813.5. Valid Loss 731559.1875\n",
      "Epoch 66. Train Loss 1215248.75. Valid Loss 731563.3125\n",
      "Epoch 67. Train Loss 1215458.0. Valid Loss 731786.25\n",
      "Epoch 68. Train Loss 1216282.875. Valid Loss 732435.125\n",
      "Epoch 69. Train Loss 1214178.5. Valid Loss 733823.75\n",
      "Epoch 70. Train Loss 1214999.0. Valid Loss 736304.6875\n",
      "Epoch 71. Train Loss 1215488.625. Valid Loss 740073.25\n",
      "Epoch 72. Train Loss 1214827.5. Valid Loss 745061.5\n",
      "Epoch 73. Train Loss 1213158.125. Valid Loss 747568.0\n",
      "Epoch 74. Train Loss 1213094.0. Valid Loss 746046.625\n",
      "Epoch 75. Train Loss 1216788.875. Valid Loss 743482.5625\n",
      "Epoch 76. Train Loss 1212561.75. Valid Loss 739883.875\n",
      "Epoch 77. Train Loss 1216629.5. Valid Loss 738526.1875\n",
      "Epoch 78. Train Loss 1211613.375. Valid Loss 736473.875\n",
      "Epoch 79. Train Loss 1212103.5. Valid Loss 734921.125\n",
      "Epoch 80. Train Loss 1216674.875. Valid Loss 734216.375\n",
      "Epoch 81. Train Loss 1211614.75. Valid Loss 733550.0\n",
      "Epoch 82. Train Loss 1216890.75. Valid Loss 733380.1875\n",
      "Epoch 83. Train Loss 1211614.0. Valid Loss 733004.0625\n",
      "Epoch 84. Train Loss 1213241.0. Valid Loss 733387.25\n",
      "Epoch 85. Train Loss 1212486.25. Valid Loss 734148.1875\n",
      "Epoch 86. Train Loss 1216101.625. Valid Loss 735785.5625\n",
      "Epoch 87. Train Loss 1214515.125. Valid Loss 737375.25\n",
      "Epoch 88. Train Loss 1212406.5. Valid Loss 739806.3125\n",
      "Epoch 89. Train Loss 1213515.25. Valid Loss 741074.0625\n",
      "Epoch 90. Train Loss 1216716.75. Valid Loss 741398.0\n",
      "Epoch 91. Train Loss 1216600.75. Valid Loss 743045.875\n",
      "Epoch 92. Train Loss 1215322.75. Valid Loss 745785.0\n",
      "Epoch 93. Train Loss 1212315.0. Valid Loss 745931.1875\n",
      "Epoch 94. Train Loss 1215856.0. Valid Loss 747301.875\n",
      "Epoch 95. Train Loss 1214528.0. Valid Loss 745514.9375\n",
      "Epoch 96. Train Loss 1212853.375. Valid Loss 741428.625\n",
      "Epoch 97. Train Loss 1213510.0. Valid Loss 738789.0\n",
      "Epoch 98. Train Loss 1216189.5. Valid Loss 737908.75\n",
      "Epoch 99. Train Loss 1216145.5. Valid Loss 737014.1875\n",
      "Epoch 100. Train Loss 1214520.0. Valid Loss 735292.875\n",
      "Epoch 101. Train Loss 1213811.125. Valid Loss 734521.0\n",
      "Epoch 102. Train Loss 1213116.5. Valid Loss 734185.625\n",
      "Epoch 103. Train Loss 1214596.0. Valid Loss 735652.0625\n",
      "Epoch 104. Train Loss 1217299.75. Valid Loss 737662.625\n",
      "Epoch 105. Train Loss 1216204.75. Valid Loss 740834.0625\n",
      "Epoch 106. Train Loss 1212875.125. Valid Loss 742681.375\n",
      "Epoch 107. Train Loss 1216535.5. Valid Loss 745454.625\n",
      "Epoch 108. Train Loss 1216580.0. Valid Loss 749523.1875\n",
      "Epoch 109. Train Loss 1211488.25. Valid Loss 749238.8125\n",
      "Epoch 110. Train Loss 1212224.875. Valid Loss 747905.0\n",
      "Epoch 111. Train Loss 1213734.75. Valid Loss 745396.125\n",
      "Epoch 112. Train Loss 1212215.125. Valid Loss 742039.3125\n",
      "Epoch 113. Train Loss 1214670.0. Valid Loss 739743.375\n",
      "Epoch 114. Train Loss 1213339.5. Valid Loss 737739.5\n",
      "Epoch 115. Train Loss 1212291.125. Valid Loss 735252.0\n",
      "Epoch 116. Train Loss 1213349.875. Valid Loss 733661.875\n",
      "Epoch 117. Train Loss 1216453.125. Valid Loss 732866.75\n",
      "Epoch 118. Train Loss 1213449.25. Valid Loss 732476.5625\n",
      "Epoch 119. Train Loss 1212806.125. Valid Loss 732962.25\n",
      "Epoch 120. Train Loss 1216086.25. Valid Loss 733852.5\n",
      "Epoch 121. Train Loss 1216351.75. Valid Loss 735749.625\n",
      "Epoch 122. Train Loss 1216949.75. Valid Loss 738914.0\n",
      "Epoch 123. Train Loss 1216502.25. Valid Loss 741450.625\n",
      "Epoch 124. Train Loss 1216765.75. Valid Loss 744240.3125\n",
      "Epoch 125. Train Loss 1214616.125. Valid Loss 746213.75\n",
      "Epoch 126. Train Loss 1216938.5. Valid Loss 747975.0\n",
      "Epoch 127. Train Loss 1211912.25. Valid Loss 748337.8125\n",
      "Epoch 128. Train Loss 1212911.25. Valid Loss 747124.375\n",
      "Epoch 129. Train Loss 1214807.5. Valid Loss 747748.4375\n",
      "Epoch 130. Train Loss 1217001.25. Valid Loss 744560.8125\n",
      "Epoch 131. Train Loss 1216114.75. Valid Loss 741208.625\n",
      "Epoch 132. Train Loss 1214539.875. Valid Loss 737304.8125\n",
      "Epoch 133. Train Loss 1215665.5. Valid Loss 736103.1875\n",
      "Epoch 134. Train Loss 1214495.0. Valid Loss 735616.8125\n",
      "Epoch 135. Train Loss 1212374.125. Valid Loss 734814.125\n",
      "Epoch 136. Train Loss 1214700.25. Valid Loss 734608.125\n",
      "Epoch 137. Train Loss 1216119.0. Valid Loss 734677.125\n",
      "Epoch 138. Train Loss 1214527.25. Valid Loss 735135.875\n",
      "Epoch 139. Train Loss 1216379.5. Valid Loss 735873.375\n",
      "Epoch 140. Train Loss 1214618.0. Valid Loss 737235.3125\n",
      "Epoch 141. Train Loss 1216467.0. Valid Loss 738313.875\n",
      "Epoch 142. Train Loss 1214864.0. Valid Loss 740734.25\n",
      "Epoch 143. Train Loss 1213865.875. Valid Loss 741749.1875\n",
      "Epoch 144. Train Loss 1214531.0. Valid Loss 743142.0625\n",
      "Epoch 145. Train Loss 1213459.25. Valid Loss 741225.375\n",
      "Epoch 146. Train Loss 1216891.5. Valid Loss 739659.5625\n",
      "Epoch 147. Train Loss 1215099.25. Valid Loss 736827.0625\n",
      "Epoch 148. Train Loss 1212874.0. Valid Loss 734337.625\n",
      "Epoch 149. Train Loss 1215754.125. Valid Loss 732411.3125\n",
      "Epoch 150. Train Loss 1211840.0. Valid Loss 731289.25\n",
      "Epoch 151. Train Loss 1213157.75. Valid Loss 731107.125\n",
      "Epoch 152. Train Loss 1215827.625. Valid Loss 731072.6875\n",
      "Epoch 153. Train Loss 1212299.125. Valid Loss 731027.0\n",
      "Epoch 154. Train Loss 1216107.5. Valid Loss 731317.4375\n",
      "Epoch 155. Train Loss 1213263.375. Valid Loss 732284.25\n",
      "Epoch 156. Train Loss 1216205.0. Valid Loss 734116.875\n",
      "Epoch 157. Train Loss 1217429.5. Valid Loss 736301.875\n",
      "Epoch 158. Train Loss 1211987.75. Valid Loss 736885.5\n",
      "Epoch 159. Train Loss 1215741.5. Valid Loss 735987.625\n",
      "Epoch 160. Train Loss 1212332.5. Valid Loss 735057.5\n",
      "Epoch 161. Train Loss 1216452.75. Valid Loss 734206.125\n",
      "Epoch 162. Train Loss 1213294.5. Valid Loss 732771.9375\n",
      "Epoch 163. Train Loss 1212682.125. Valid Loss 732113.4375\n",
      "Epoch 164. Train Loss 1214544.0. Valid Loss 731626.875\n",
      "Epoch 165. Train Loss 1212780.75. Valid Loss 731506.75\n",
      "Epoch 166. Train Loss 1214674.875. Valid Loss 732080.5625\n",
      "Epoch 167. Train Loss 1216479.0. Valid Loss 733398.375\n",
      "Epoch 168. Train Loss 1216209.5. Valid Loss 736378.375\n",
      "Epoch 169. Train Loss 1214879.25. Valid Loss 743454.5\n",
      "Epoch 170. Train Loss 1216972.0. Valid Loss 755132.0625\n",
      "Epoch 171. Train Loss 1216212.875. Valid Loss 767750.0\n",
      "Epoch 172. Train Loss 1211479.25. Valid Loss 776325.8125\n",
      "Epoch 173. Train Loss 1215711.25. Valid Loss 776997.25\n",
      "Epoch 174. Train Loss 1216595.5. Valid Loss 770590.4375\n",
      "Epoch 175. Train Loss 1213520.875. Valid Loss 757927.5\n",
      "Epoch 176. Train Loss 1216569.25. Valid Loss 745773.5625\n",
      "Epoch 177. Train Loss 1213055.75. Valid Loss 738020.875\n",
      "Epoch 178. Train Loss 1214675.625. Valid Loss 734062.875\n",
      "Epoch 179. Train Loss 1211550.75. Valid Loss 732188.0\n",
      "Epoch 180. Train Loss 1216964.75. Valid Loss 731616.4375\n",
      "Epoch 181. Train Loss 1214243.5. Valid Loss 731340.75\n",
      "Epoch 182. Train Loss 1215715.0. Valid Loss 731583.5\n",
      "Epoch 183. Train Loss 1215712.75. Valid Loss 732376.8125\n",
      "Epoch 184. Train Loss 1215882.75. Valid Loss 734622.875\n",
      "Epoch 185. Train Loss 1212256.625. Valid Loss 737144.875\n",
      "Epoch 186. Train Loss 1215881.0. Valid Loss 740366.0\n",
      "Epoch 187. Train Loss 1215914.25. Valid Loss 744840.8125\n",
      "Epoch 188. Train Loss 1213691.25. Valid Loss 749586.875\n",
      "Epoch 189. Train Loss 1216870.75. Valid Loss 751558.0625\n",
      "Epoch 190. Train Loss 1214661.75. Valid Loss 749294.75\n",
      "Epoch 191. Train Loss 1211510.75. Valid Loss 747560.5\n",
      "Epoch 192. Train Loss 1211579.75. Valid Loss 744174.25\n",
      "Epoch 193. Train Loss 1215813.5. Valid Loss 740272.75\n",
      "Epoch 194. Train Loss 1214146.0. Valid Loss 736844.75\n",
      "Epoch 195. Train Loss 1216934.5. Valid Loss 735874.0\n",
      "Epoch 196. Train Loss 1214822.25. Valid Loss 735281.0\n",
      "Epoch 197. Train Loss 1213282.0. Valid Loss 734498.5625\n",
      "Epoch 198. Train Loss 1213412.5. Valid Loss 734497.0\n",
      "Epoch 199. Train Loss 1213390.25. Valid Loss 734420.3125\n",
      "Epoch 200. Train Loss 1216768.5. Valid Loss 733667.9375\n",
      "Epoch 201. Train Loss 1215764.625. Valid Loss 733713.125\n",
      "Epoch 202. Train Loss 1216382.0. Valid Loss 733608.5625\n",
      "Epoch 203. Train Loss 1212246.75. Valid Loss 732824.625\n",
      "Epoch 204. Train Loss 1214284.0. Valid Loss 731643.3125\n",
      "Epoch 205. Train Loss 1212229.5. Valid Loss 731741.5\n",
      "Epoch 206. Train Loss 1213387.125. Valid Loss 732080.125\n",
      "Epoch 207. Train Loss 1212965.5. Valid Loss 732362.375\n",
      "Epoch 208. Train Loss 1213397.625. Valid Loss 732871.875\n",
      "Epoch 209. Train Loss 1217088.25. Valid Loss 733676.875\n",
      "Epoch 210. Train Loss 1216187.75. Valid Loss 734944.375\n",
      "Epoch 211. Train Loss 1216948.5. Valid Loss 737185.1875\n",
      "Epoch 212. Train Loss 1214580.5. Valid Loss 738205.25\n",
      "Epoch 213. Train Loss 1215173.875. Valid Loss 738523.9375\n",
      "Epoch 214. Train Loss 1213390.75. Valid Loss 737822.5\n",
      "Epoch 215. Train Loss 1212950.875. Valid Loss 736992.875\n",
      "Epoch 216. Train Loss 1216948.75. Valid Loss 737276.5625\n",
      "Epoch 217. Train Loss 1216263.125. Valid Loss 739790.75\n",
      "Epoch 218. Train Loss 1216457.75. Valid Loss 741540.375\n",
      "Epoch 219. Train Loss 1216906.25. Valid Loss 741332.0\n",
      "Epoch 220. Train Loss 1217117.5. Valid Loss 739634.625\n",
      "Epoch 221. Train Loss 1214792.25. Valid Loss 737511.875\n",
      "Epoch 222. Train Loss 1216443.375. Valid Loss 734805.1875\n",
      "Epoch 223. Train Loss 1214776.0. Valid Loss 733921.375\n",
      "Epoch 224. Train Loss 1216027.5. Valid Loss 734304.3125\n",
      "Epoch 225. Train Loss 1212484.875. Valid Loss 734353.0\n",
      "Epoch 226. Train Loss 1214510.5. Valid Loss 733936.625\n",
      "Epoch 227. Train Loss 1213628.25. Valid Loss 733441.9375\n",
      "Epoch 228. Train Loss 1214079.0. Valid Loss 733001.4375\n",
      "Epoch 229. Train Loss 1216303.75. Valid Loss 733242.75\n",
      "Epoch 230. Train Loss 1212331.25. Valid Loss 734247.0625\n",
      "Epoch 231. Train Loss 1216255.5. Valid Loss 735876.5625\n",
      "Epoch 232. Train Loss 1213490.75. Valid Loss 736945.75\n",
      "Epoch 233. Train Loss 1213881.5. Valid Loss 738691.5\n",
      "Epoch 234. Train Loss 1215314.75. Valid Loss 740604.0625\n",
      "Epoch 235. Train Loss 1211591.125. Valid Loss 740906.625\n",
      "Epoch 236. Train Loss 1211939.5. Valid Loss 740869.875\n",
      "Epoch 237. Train Loss 1216661.375. Valid Loss 739160.375\n",
      "Epoch 238. Train Loss 1215395.75. Valid Loss 735830.125\n",
      "Epoch 239. Train Loss 1212739.5. Valid Loss 734308.5\n",
      "Epoch 240. Train Loss 1213374.0. Valid Loss 733548.25\n",
      "Epoch 241. Train Loss 1215346.5. Valid Loss 735310.125\n",
      "Epoch 242. Train Loss 1214791.25. Valid Loss 738997.5\n",
      "Epoch 243. Train Loss 1217316.375. Valid Loss 744606.625\n",
      "Epoch 244. Train Loss 1214032.0. Valid Loss 748417.9375\n",
      "Epoch 245. Train Loss 1213344.375. Valid Loss 748945.125\n",
      "Epoch 246. Train Loss 1216631.75. Valid Loss 750269.375\n",
      "Epoch 247. Train Loss 1216998.125. Valid Loss 749701.3125\n",
      "Epoch 248. Train Loss 1212755.5. Valid Loss 746597.0\n",
      "Epoch 249. Train Loss 1215080.0. Valid Loss 745151.875\n",
      "Epoch 250. Train Loss 1213548.375. Valid Loss 741641.0\n",
      "Epoch 251. Train Loss 1216479.875. Valid Loss 739547.0625\n",
      "Epoch 252. Train Loss 1216939.75. Valid Loss 741367.5\n",
      "Epoch 253. Train Loss 1213049.0. Valid Loss 741823.1875\n",
      "Epoch 254. Train Loss 1216623.25. Valid Loss 743526.625\n",
      "Epoch 255. Train Loss 1216520.125. Valid Loss 743080.0\n",
      "Epoch 256. Train Loss 1213727.375. Valid Loss 741406.625\n",
      "Epoch 257. Train Loss 1216524.375. Valid Loss 741260.0\n",
      "Epoch 258. Train Loss 1212861.375. Valid Loss 740696.125\n",
      "Epoch 259. Train Loss 1211539.25. Valid Loss 739983.9375\n",
      "Epoch 260. Train Loss 1213895.75. Valid Loss 740408.5625\n",
      "Epoch 261. Train Loss 1213566.875. Valid Loss 738705.25\n",
      "Epoch 262. Train Loss 1212231.75. Valid Loss 736098.8125\n",
      "Epoch 263. Train Loss 1212069.25. Valid Loss 733939.125\n",
      "Epoch 264. Train Loss 1215838.625. Valid Loss 733075.5\n",
      "Epoch 265. Train Loss 1211532.25. Valid Loss 732663.75\n",
      "Epoch 266. Train Loss 1211352.5. Valid Loss 732203.8125\n",
      "Epoch 267. Train Loss 1213322.375. Valid Loss 731687.9375\n",
      "Epoch 268. Train Loss 1212376.25. Valid Loss 731736.3125\n",
      "Epoch 269. Train Loss 1212311.25. Valid Loss 732156.0\n",
      "Epoch 270. Train Loss 1212509.625. Valid Loss 732979.75\n",
      "Epoch 271. Train Loss 1215987.75. Valid Loss 733413.3125\n",
      "Epoch 272. Train Loss 1216905.375. Valid Loss 734397.0625\n",
      "Epoch 273. Train Loss 1216718.0. Valid Loss 736777.3125\n",
      "Epoch 274. Train Loss 1214451.375. Valid Loss 737212.5625\n",
      "Epoch 275. Train Loss 1212761.375. Valid Loss 737067.4375\n",
      "Epoch 276. Train Loss 1214007.25. Valid Loss 736488.5\n",
      "Epoch 277. Train Loss 1213014.5. Valid Loss 736229.625\n",
      "Epoch 278. Train Loss 1213735.125. Valid Loss 735818.0\n",
      "Epoch 279. Train Loss 1216681.125. Valid Loss 735884.375\n",
      "Epoch 280. Train Loss 1211982.75. Valid Loss 735376.9375\n",
      "Epoch 281. Train Loss 1215341.75. Valid Loss 735274.625\n",
      "Epoch 282. Train Loss 1216295.75. Valid Loss 736308.3125\n",
      "Epoch 283. Train Loss 1212275.25. Valid Loss 736220.0\n",
      "Epoch 284. Train Loss 1215355.875. Valid Loss 735958.75\n",
      "Epoch 285. Train Loss 1216955.5. Valid Loss 735489.0\n",
      "Epoch 286. Train Loss 1215997.5. Valid Loss 736601.625\n",
      "Epoch 287. Train Loss 1216354.375. Valid Loss 739968.25\n",
      "Epoch 288. Train Loss 1213203.875. Valid Loss 742205.8125\n",
      "Epoch 289. Train Loss 1214074.0. Valid Loss 741297.875\n",
      "Epoch 290. Train Loss 1216618.75. Valid Loss 741675.25\n",
      "Epoch 291. Train Loss 1216938.5. Valid Loss 742421.25\n",
      "Epoch 292. Train Loss 1215788.625. Valid Loss 741888.625\n",
      "Epoch 293. Train Loss 1217040.5. Valid Loss 741322.625\n",
      "Epoch 294. Train Loss 1213337.0. Valid Loss 740435.25\n",
      "Epoch 295. Train Loss 1216494.875. Valid Loss 740284.4375\n",
      "Epoch 296. Train Loss 1214484.75. Valid Loss 741695.375\n",
      "Epoch 297. Train Loss 1216879.875. Valid Loss 740138.75\n",
      "Epoch 298. Train Loss 1213327.5. Valid Loss 737521.5\n",
      "Epoch 299. Train Loss 1214252.75. Valid Loss 736137.875\n",
      "Epoch 300. Train Loss 1215696.0. Valid Loss 735280.125\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    predictor.train()\n",
    "\n",
    "    loss_value = 0\n",
    "    for x1_train, x2_train, features, mol_id, cid, sig in train_loader:\n",
    "        x1_train = x1_train.to(dev)\n",
    "        x2_train = x2_train.to(dev)\n",
    "        features = features.to(dev)\n",
    "        \n",
    "        if x1_train.shape[0] == 1:\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x2_pred = predictor(x1_train, features)\n",
    "        loss = F.mse_loss(x2_train, x2_pred, reduction=\"sum\")\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss_value = 0\n",
    "        for x1_data, x2_data, mol_features, mol_id, cid, sig in valid_loader:\n",
    "            data_x2_pred = predictor(x1_data.to(dev), mol_features.to(dev))\n",
    "            valid_loss_value += F.mse_loss(x2_data.to(dev), data_x2_pred, reduction=\"sum\").item()\n",
    "        print(f\"Epoch {i+1}. Train Loss {loss_value / len(train_loader)}. Valid Loss {valid_loss_value / len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "fd27fda0-d38a-4041-8c90-da3363b2c630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "predictor.eval()\n",
    "with torch.no_grad():\n",
    "    for x1_data, x2_data, mol_features, mol_id, cid, sig in test_loader:\n",
    "        x1_data = x1_data.to(dev)\n",
    "        x2_data = x2_data.to(dev)\n",
    "        delta_x = (x2_data - x1_data).data.cpu().numpy().astype(float)\n",
    "        x2_pred = predictor(x1_data, mol_features.to(dev))\n",
    "        delta_x_pred = (x2_pred - x1_data).data.cpu().numpy().astype(float)\n",
    "        result = 0\n",
    "        for i in range(delta_x.shape[0]):\n",
    "            results[sig[i]] = get_metric_func('pearson')(delta_x[i, :], delta_x_pred[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "ed4b7019-b327-40c3-b3f7-ffe1f104f34c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A549_13': 0.1103346867166929,\n",
       " 'PC3_6': 0.08399428564030627,\n",
       " 'A549_6': -0.028775688029225685,\n",
       " 'VCAP_9': 0.10153353105803228,\n",
       " 'A375_6': 0.024216956296907496,\n",
       " 'HA1E_6': 0.03853736720090515,\n",
       " 'HA1E_9': 0.01084647689216178,\n",
       " 'HT29_6': 0.028279612293074942,\n",
       " 'VCAP_6': 0.08407544691831219,\n",
       " 'HT29_9': 0.032128426369930976,\n",
       " 'MCF7_9': 0.06877149542887703,\n",
       " 'A549_9': 0.10518967302194415,\n",
       " 'ASC_13': 0.12036072939785611,\n",
       " 'A375_9': 0.05682655514783971,\n",
       " 'MCF7_6': 0.04910575153115867,\n",
       " 'HA1E_13': 0.028087619225809563,\n",
       " 'PC3_9': 0.09485917464881406}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c501287-52a3-4329-b504-74c1c07aaa2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====load vae for x1 and x2=======\n"
     ]
    }
   ],
   "source": [
    "if init_mode == 'pretrain_shRNA':\n",
    "    # Carregando modelo pré-treinado\n",
    "    print('=====load vae for x1 and x2=======')\n",
    "    model_dict = model.state_dict()\n",
    "    filename = '../results/trained_model_shRNA_vae_x1/best_model.pt'\n",
    "    model_base_x1 = torch.load(filename, map_location='cpu')\n",
    "    model_base_x1_dict = model_base_x1.state_dict()\n",
    "    for k in model_dict.keys():\n",
    "        if k in model_base_x1_dict.keys():\n",
    "            model_dict[k] = model_base_x1_dict[k]\n",
    "    filename = '../results/trained_model_shRNA_vae_x2/best_model.pt'\n",
    "    model_base_x2 = torch.load(filename, map_location='cpu')\n",
    "    model_base_x2_dict = model_base_x2.state_dict()\n",
    "    for k in model_dict.keys():\n",
    "        if k in model_base_x2_dict.keys():\n",
    "            model_dict[k] = model_base_x2_dict[k]\n",
    "    model.load_state_dict(model_dict)\n",
    "    del model_base_x1, model_base_x2\n",
    "else:\n",
    "    epoch_hist, best_epoch = model.train_model(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=valid_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        save_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca090944-eb92-449f-bee3-2f83da6e49d7",
   "metadata": {},
   "source": [
    "## Avaliação do Modelo no Conjunto Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "efcb6c6c-036b-416f-ad18-161ea87060ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, _, test_metrics_dict_ls = model.test_model(loader=test_loader, metrics_func=['pearson', 'rmse', 'precision100'])\n",
    "\n",
    "for name, rec_dict_value in zip(['test'], [test_metrics_dict_ls]):\n",
    "    df_rec = pd.DataFrame.from_dict(rec_dict_value)\n",
    "    smi_ls = []\n",
    "    for smi_id in df_rec['cp_id']:\n",
    "        smi_ls.append(idx2smi[smi_id])\n",
    "    df_rec['canonical_smiles'] = smi_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3d14d32-fa41-4218-966b-beafb48d4781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x1_rec_pearson', 'x1_rec_rmse', 'x1_rec_neg_precision100',\n",
       "       'x1_rec_pos_precision100', 'x2_rec_pearson', 'x2_rec_rmse',\n",
       "       'x2_rec_neg_precision100', 'x2_rec_pos_precision100', 'x2_pred_pearson',\n",
       "       'x2_pred_rmse', 'x2_pred_neg_precision100', 'x2_pred_pos_precision100',\n",
       "       'DEG_rec_pearson', 'DEG_rec_rmse', 'DEG_rec_neg_precision100',\n",
       "       'DEG_rec_pos_precision100', 'DEG_pred_pearson', 'DEG_pred_rmse',\n",
       "       'DEG_pred_neg_precision100', 'DEG_pred_pos_precision100', 'cp_id',\n",
       "       'cid', 'sig', 'canonical_smiles'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe449323-0342-4c90-a1f1-eb1ddec50059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
